Wed Apr 13 23:15:13 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-SXM4-40GB      On   | 00000000:52:00.0 Off |                    0 |
| N/A   28C    P0    56W / 400W |   2988MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  A100-SXM4-40GB      On   | 00000000:57:00.0 Off |                    0 |
| N/A   29C    P0    58W / 400W |   3142MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  A100-SXM4-40GB      On   | 00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |   3240MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  A100-SXM4-40GB      On   | 00000000:73:00.0 Off |                    0 |
| N/A   28C    P0    57W / 400W |   3142MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   4  A100-SXM4-40GB      On   | 00000000:92:00.0 Off |                    0 |
| N/A   28C    P0    58W / 400W |   3182MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   5  A100-SXM4-40GB      On   | 00000000:97:00.0 Off |                    0 |
| N/A   28C    P0    57W / 400W |   3240MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   6  A100-SXM4-40GB      On   | 00000000:AD:00.0 Off |                    0 |
| N/A   28C    P0    57W / 400W |   3142MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   7  A100-SXM4-40GB      On   | 00000000:B3:00.0 Off |                    0 |
| N/A   29C    P0    58W / 400W |   2998MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 3
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 6
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 2
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 7
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 1
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 5
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 4
| distributed init done!
| distributed init done!
| initialized host 3451ffe1af3b as rank 0 and device id 0
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=0, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=None, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| /data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(33712, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (maybe_ln1): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (maybe_ln2): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(33712, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (self_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (encoder_attn_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): MaybeLayerNorm(
          (ln): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
  )
)
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 8 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=3 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Command line:
INFO:Args:
INFO:/workspace/translation/train.py --local_rank=0 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=0, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=3, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=3, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=3, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=4 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=7 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Command line:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=4, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=4, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=4, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:/workspace/translation/train.py --local_rank=6 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=5 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Command line:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=7, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=7, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=7, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Args:
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=2 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=6, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=6, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=6, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:/workspace/translation/train.py --local_rank=1 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=2, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=2, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=2, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=5, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=5, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=5, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=1, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=1, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=1, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
prune_ratios[module.decoder.layers.0.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.0.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.0.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.0.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.0.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.0.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.0.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.0.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.0.fc1.weight]:0.8
prune_ratios[module.decoder.layers.0.fc2.weight]:0.8
prune_ratios[module.decoder.layers.1.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.1.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.1.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.1.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.1.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.1.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.1.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.1.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.1.fc1.weight]:0.8
prune_ratios[module.decoder.layers.1.fc2.weight]:0.8
prune_ratios[module.decoder.layers.2.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.2.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.2.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.2.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.2.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.2.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.2.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.2.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.2.fc1.weight]:0.8
prune_ratios[module.decoder.layers.2.fc2.weight]:0.8
prune_ratios[module.decoder.layers.3.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.3.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.3.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.3.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.3.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.3.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.3.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.3.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.3.fc1.weight]:0.8
prune_ratios[module.decoder.layers.3.fc2.weight]:0.8
prune_ratios[module.decoder.layers.4.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.4.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.4.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.4.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.4.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.4.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.4.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.4.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.4.fc1.weight]:0.8
prune_ratios[module.decoder.layers.4.fc2.weight]:0.8
prune_ratios[module.decoder.layers.5.self_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.5.self_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.5.self_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.5.self_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.5.encoder_attn.in_proj_weight_q]:0.8
prune_ratios[module.decoder.layers.5.encoder_attn.in_proj_weight_k]:0.8
prune_ratios[module.decoder.layers.5.encoder_attn.in_proj_weight_v]:0.8
prune_ratios[module.decoder.layers.5.encoder_attn.out_proj.weight]:0.8
prune_ratios[module.decoder.layers.5.fc1.weight]:0.8
prune_ratios[module.decoder.layers.5.fc2.weight]:0.8
Hardened weight sparsity: name, num_nonzeros, total_num, sparsity
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
module.decoder.layers.0.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
INFO:Initializing ADMM pruning algorithm
INFO:ADMM rho is set to 0.001
INFO:Hard prune
module.decoder.layers.0.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.0.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.0.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.0.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.0.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.0.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.0.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.0.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.0.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.1.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.1.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.1.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.1.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.1.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.1.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.1.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.1.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.1.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.1.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.2.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.2.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.2.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.2.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.2.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.2.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.2.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.2.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.2.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.2.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.3.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.3.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.3.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.3.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.3.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.3.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.3.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.3.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.3.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.3.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.4.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.4.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.4.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.4.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.4.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.4.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.4.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.4.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.4.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.4.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.5.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.5.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.5.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.5.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.5.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.5.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.5.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.5.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.5.fc1.weight: 838310, 4194304, 0.8001313209533691
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=4 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=4, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=4, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=4, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=5 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=5, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=5, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=5, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=3 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=3, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=3, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=3, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=2 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=2, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=2, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=2, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
module.decoder.layers.5.fc2.weight: 837672, 4194304, 0.8002834320068359
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=0 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=0, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=1 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=1, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=1, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=1, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=6 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=6, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=6, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=6, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Command line:
INFO:/workspace/translation/train.py --local_rank=7 /data/wmt14_en_de_joined_dict --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas (0.9, 0.997) --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 4000 --lr 0.000846 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --seed 3 --max-epoch 2 --fuse-layer-norm --online-eval --log-interval 500 --save-dir results/tmp//step_0/ --stat-file DGX1_amp_8GPU.json --distributed-init-method env:// --amp --amp-level O2 --restart-training --resume None --sp-retrain --sp-prune-before-retrain --sp-admm-sparsity-type irregular --sp-config-file profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml --no-epoch-checkpoints
INFO:Args:
INFO:Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, admm_debug=False, amp=True, amp_level='O2', apply_att_mask=False, apply_dynamic_att_mask=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, combine_without_finetune=False, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=7, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=7, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', generate_rand_seq_gap_yaml=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, load_parallel_model=False, local_rank=7, log_interval=500, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=2, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, mhatt_debug=False, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_gap_partition=6, num_shards=1, nv_sparse=False, online_eval=True, optimizer='adam', output_compressed_format=False, pad_sequence=1, padding_idx=1, parallel_1_dir=None, parallel_2_dir=None, parallel_3_dir=None, parallel_4_dir=None, parallel_5_dir=None, parallel_6_dir=None, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quick_save=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restart_training=True, restore_file='checkpoint_last.pt', resume='None', retrain_mask_pattern='weight', retrain_mask_seed=None, retrain_mask_sparsity=-1.0, sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='results/tmp//step_0/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=3, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, short_train=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', sp_admm=False, sp_admm_block='(1,)', sp_admm_buckets_num=16, sp_admm_data_format=None, sp_admm_do_not_permute_conv=False, sp_admm_elem_per_row=1, sp_admm_fixed_params=False, sp_admm_lr=0.001, sp_admm_multi=False, sp_admm_pattern_col_sub=4, sp_admm_pattern_row_sub=1, sp_admm_rho=None, sp_admm_select_number=4, sp_admm_sparsity_type='irregular', sp_admm_tile=None, sp_admm_update_batch=None, sp_admm_update_epoch=1, sp_backbone=False, sp_block_irregular_sparsity='(0,0)', sp_config_file='profiles/3_step_forward_gap/0.8_std_naming//step_0.yaml', sp_global_magnitude=False, sp_global_weight_sparsity=-1, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_grad_update=False, sp_gs_output_ptr=None, sp_gs_output_v=None, sp_lars=False, sp_lars_trust_coef=0.001, sp_load_frozen_weights=None, sp_load_prune_params=None, sp_mask_update_freq=10, sp_no_harden=False, sp_pre_defined_mask_dir=None, sp_prune_before_retrain=True, sp_prune_threshold=-1.0, sp_retrain=True, sp_retrain_multi=False, sp_store_prune_params=None, sp_store_weights=None, sp_subset_progressive=False, sp_update_init_method='weight', src_vocab_size=33712, stat_file='DGX1_amp_8GPU.json', target_bleu=0.0, target_lang='de', test_cased_bleu=False, tgt_vocab_size=33712, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_q to module.decoder.layers.0.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_k to module.decoder.layers.0.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.self_attn.in_proj_weight_v to module.decoder.layers.0.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.self_attn.out_proj.weight to module.decoder.layers.0.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_q to module.decoder.layers.0.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_k to module.decoder.layers.0.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.encoder_attn.in_proj_weight_v to module.decoder.layers.0.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.0.encoder_attn.out_proj.weight to module.decoder.layers.0.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.0.fc1.weight to module.decoder.layers.0.fc1.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.0.fc2.weight to module.decoder.layers.0.fc2.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_q to module.decoder.layers.1.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_k to module.decoder.layers.1.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.self_attn.in_proj_weight_v to module.decoder.layers.1.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.1.self_attn.out_proj.weight to module.decoder.layers.1.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_q to module.decoder.layers.1.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_k to module.decoder.layers.1.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.encoder_attn.in_proj_weight_v to module.decoder.layers.1.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.encoder_attn.out_proj.weight to module.decoder.layers.1.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.1.fc1.weight to module.decoder.layers.1.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.1.fc2.weight to module.decoder.layers.1.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_q to module.decoder.layers.2.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_k to module.decoder.layers.2.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.self_attn.in_proj_weight_v to module.decoder.layers.2.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.self_attn.out_proj.weight to module.decoder.layers.2.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_q to module.decoder.layers.2.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_k to module.decoder.layers.2.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.encoder_attn.in_proj_weight_v to module.decoder.layers.2.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.2.encoder_attn.out_proj.weight to module.decoder.layers.2.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.fc1.weight to module.decoder.layers.2.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.2.fc2.weight to module.decoder.layers.2.fc2.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_q to module.decoder.layers.3.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_k to module.decoder.layers.3.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.3.self_attn.in_proj_weight_v to module.decoder.layers.3.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.self_attn.out_proj.weight to module.decoder.layers.3.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_q to module.decoder.layers.3.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_k to module.decoder.layers.3.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.3.encoder_attn.in_proj_weight_v to module.decoder.layers.3.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.encoder_attn.out_proj.weight to module.decoder.layers.3.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.3.fc1.weight to module.decoder.layers.3.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.3.fc2.weight to module.decoder.layers.3.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_q to module.decoder.layers.4.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_k to module.decoder.layers.4.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.self_attn.in_proj_weight_v to module.decoder.layers.4.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.self_attn.out_proj.weight to module.decoder.layers.4.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_q to module.decoder.layers.4.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_k to module.decoder.layers.4.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.4.encoder_attn.in_proj_weight_v to module.decoder.layers.4.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.4.encoder_attn.out_proj.weight to module.decoder.layers.4.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.4.fc1.weight to module.decoder.layers.4.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.4.fc2.weight to module.decoder.layers.4.fc2.weight
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_q to module.decoder.layers.5.self_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_k to module.decoder.layers.5.self_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.self_attn.in_proj_weight_v to module.decoder.layers.5.self_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.self_attn.out_proj.weight to module.decoder.layers.5.self_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_q to module.decoder.layers.5.encoder_attn.in_proj_weight_q
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_k to module.decoder.layers.5.encoder_attn.in_proj_weight_k
INFO:Map weight config name from decoder.layers.5.encoder_attn.in_proj_weight_v to module.decoder.layers.5.encoder_attn.in_proj_weight_v
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
module.decoder.layers.0.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
INFO:Map weight config name from decoder.layers.5.encoder_attn.out_proj.weight to module.decoder.layers.5.encoder_attn.out_proj.weight
module.decoder.layers.0.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.0.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.0.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
INFO:Map weight config name from decoder.layers.5.fc1.weight to module.decoder.layers.5.fc1.weight
module.decoder.layers.0.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.0.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
INFO:Map weight config name from decoder.layers.5.fc2.weight to module.decoder.layers.5.fc2.weight
module.decoder.layers.0.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.0.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.0.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.0.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.1.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.1.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.1.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.1.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.1.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.1.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.1.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.1.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.1.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.1.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.2.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.2.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.2.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.2.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.2.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.2.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.2.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.2.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.2.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.2.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.3.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.3.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.3.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.3.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.3.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.3.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.3.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.3.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.3.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.3.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.4.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.4.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.4.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.4.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.4.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.4.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.4.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.4.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.4.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.4.fc2.weight: 837672, 4194304, 0.8002834320068359
module.decoder.layers.5.self_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.5.self_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.5.self_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.5.self_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.5.encoder_attn.in_proj_weight_q: 209601, 1048576, 0.8001089096069336
module.decoder.layers.5.encoder_attn.in_proj_weight_k: 209165, 1048576, 0.8005247116088867
module.decoder.layers.5.encoder_attn.in_proj_weight_v: 209331, 1048576, 0.8003664016723633
module.decoder.layers.5.encoder_attn.out_proj.weight: 209330, 1048576, 0.8003673553466797
module.decoder.layers.5.fc1.weight: 838310, 4194304, 0.8001313209533691
module.decoder.layers.5.fc2.weight: 837672, 4194304, 0.8002834320068359
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
module.encoder.embed_tokens.weight: 34520037, 34521088, (33712, 1024), [3.0445158623049196e-05]
module.encoder.layers.0.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.0.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.0.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.0.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.0.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.0.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.0.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.0.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.0.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.0.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.0.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.0.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.1.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.1.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.1.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.1.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.1.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.1.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.1.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.1.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.1.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.1.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.1.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.1.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.2.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.2.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.2.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.2.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.2.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.2.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.2.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.2.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.2.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.2.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.2.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.2.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.3.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.3.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.3.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.3.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.3.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.3.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.3.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.3.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.3.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.3.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.3.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.3.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.4.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.4.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.4.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.4.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.4.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.4.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.4.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.4.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.4.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.4.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.4.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.4.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.5.self_attn.in_proj_weight_q: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.5.self_attn.in_proj_weight_k: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.5.self_attn.in_proj_weight_v: 1048576, 1048576, (1024, 1024), [0.0]
module.encoder.layers.5.self_attn.out_proj.weight: 1048574, 1048576, (1024, 1024), [1.9073486328125e-06]
module.encoder.layers.5.fc1.weight: 4194301, 4194304, (4096, 1024), [7.152557373046875e-07]
module.encoder.layers.5.fc1.bias: 0, 4096, (4096,), [1.0]
module.encoder.layers.5.fc2.weight: 4194299, 4194304, (1024, 4096), [1.1920928955078125e-06]
module.encoder.layers.5.fc2.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.5.maybe_ln1.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.5.maybe_ln1.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layers.5.maybe_ln2.ln.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layers.5.maybe_ln2.ln.bias: 0, 1024, (1024,), [1.0]
module.encoder.layer_norm.weight: 1024, 1024, (1024,), [0.0]
module.encoder.layer_norm.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.0.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.0.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.0.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.0.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.0.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.0.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.0.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.0.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.0.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.0.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.0.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.0.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.0.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.0.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.0.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.0.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.0.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.0.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.1.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.1.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.1.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.1.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.1.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.1.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.1.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.1.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.1.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.1.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.1.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.1.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.1.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.1.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.1.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.1.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.1.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.1.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.2.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.2.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.2.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.2.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.2.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.2.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.2.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.2.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.2.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.2.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.2.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.2.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.2.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.2.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.2.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.2.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.2.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.2.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.3.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.3.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.3.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.3.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.3.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.3.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.3.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.3.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.3.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.3.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.3.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.3.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.3.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.3.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.3.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.3.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.3.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.3.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.4.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.4.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.4.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.4.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.4.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.4.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.4.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.4.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.4.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.4.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.4.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.4.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.4.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.4.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.4.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.4.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.4.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.4.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.5.self_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.5.self_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.5.self_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.5.self_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.5.self_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.5.self_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.5.encoder_attn.in_proj_weight_q: 209601, 1048576, (1024, 1024), [0.8001089096069336]
module.decoder.layers.5.encoder_attn.in_proj_weight_k: 209165, 1048576, (1024, 1024), [0.8005247116088867]
module.decoder.layers.5.encoder_attn.in_proj_weight_v: 209331, 1048576, (1024, 1024), [0.8003664016723633]
module.decoder.layers.5.encoder_attn.out_proj.weight: 209330, 1048576, (1024, 1024), [0.8003673553466797]
module.decoder.layers.5.encoder_attn_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.5.encoder_attn_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.5.fc1.weight: 838310, 4194304, (4096, 1024), [0.8001313209533691]
module.decoder.layers.5.fc1.bias: 0, 4096, (4096,), [1.0]
module.decoder.layers.5.fc2.weight: 837672, 4194304, (1024, 4096), [0.8002834320068359]
module.decoder.layers.5.fc2.bias: 0, 1024, (1024,), [1.0]
module.decoder.layers.5.final_layer_norm.ln.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layers.5.final_layer_norm.ln.bias: 0, 1024, (1024,), [1.0]
module.decoder.layer_norm.weight: 1024, 1024, (1024,), [0.0]
module.decoder.layer_norm.bias: 0, 1024, (1024,), [1.0]
[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]
0.0 0.0
0 2
0 inf
0.0 inf
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Transformer | epoch 0 | step 500 |avg loss 12.022 |avg tokens 35975.878 |tokens/s 394727.205 |walltime 74.850 |
Transformer | epoch 0 | step 1000 |avg loss 9.483 |avg tokens 36339.634 |tokens/s 324557.390 |walltime 130.833 |
Transformer | epoch 0 | step 1500 |avg loss 7.846 |avg tokens 36122.906 |tokens/s 395321.720 |walltime 176.521 |
Transformer | epoch 0 | step 2000 |avg loss 6.789 |avg tokens 36196.532 |tokens/s 400323.519 |walltime 221.730 |
